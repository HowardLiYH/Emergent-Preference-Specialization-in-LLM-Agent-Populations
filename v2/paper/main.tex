% =============================================================================
% EMERGENT TOOL SPECIALIZATION IN LLM AGENT POPULATIONS v2
% NeurIPS 2025/2026 Submission
% =============================================================================
% Main body: 10 pages max (excluding references and appendix)
% Appendix: Unlimited
% =============================================================================

\documentclass[11pt]{article}

% NeurIPS 2025 style
\usepackage[final]{neurips_2025}

% Essential packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{enumitem}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

% Custom commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}

% =============================================================================
% TITLE AND AUTHORS
% =============================================================================
\title{Emergent Tool Specialization in LLM Agent Populations\\Through Competitive Selection}

\author{
  Yuhao Li \\
  University of Pennsylvania\\
  \texttt{li88@sas.upenn.edu}
}

\begin{document}

\maketitle

% =============================================================================
% ABSTRACT
% =============================================================================
\begin{abstract}
We extend emergent preference specialization to \textit{tool-based capability levels}, demonstrating that LLM agent populations spontaneously develop specialized tool expertise through competition alone. Unlike prior work using synthetic rule domains, we define five concrete tool levels: L0 (base LLM), L1 (Python execution), L2 (vision), L3 (RAG/retrieval), and L4 (web access)---each providing measurable capability gains. Using Thompson Sampling for tool discovery, agents learn \textit{which tools to use for which regimes} without prior knowledge of optimal mappings. We introduce \textbf{non-uniform regime distributions} with varying frequencies and rewards, proving that equilibrium specialist counts follow $n_r \propto (f_r \times R_r \times D_r)^{2/3}$ (Theorem 4). Our \textbf{agent memory system} enables experience accumulation with anti-leakage guarantees (strategies, not answers). The \textbf{Emergent Specialization Benchmark (ESB)} validates tool necessity: L0 baselines achieve only 20\% on L1 tasks vs. 95\% with tools. Cost comparison shows our population approach uses 40-60\% fewer tokens than bandit and PPO baselines to reach equivalent performance. Stochasticity analysis confirms emergence: 8+ unique specialization patterns across 10 seeds. All code, data, and figures are reproducible.
\end{abstract}

% =============================================================================
% SECTION 1: INTRODUCTION
% =============================================================================
\section{Introduction}
\label{sec:intro}

Prior work~\cite{li2025preference} demonstrated that LLM agents develop specialized \textit{preferences} through competition, using synthetic rule domains as a proof-of-concept. While theoretically compelling, the synthetic rules (e.g., ``select word starting with vowel'') are far removed from practical applications. This work addresses a key question: \textbf{Can emergent specialization produce practically useful tool expertise?}

We answer affirmatively by reformulating capability levels as \textit{tool access}:

\begin{center}
\begin{tabular}{clc}
\toprule
\textbf{Level} & \textbf{Tool} & \textbf{Capability} \\
\midrule
L0 & Base LLM & Text completion only \\
L1 & Python & Code execution \\
L2 & Vision & Image analysis \\
L3 & RAG & Document retrieval \\
L4 & Web & Real-time data access \\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Key Insight.} We establish a \textit{hidden 1:1 mapping} between task regimes and optimal tools. Agents must \textit{discover} this mapping through Thompson Sampling---the optimal tool for ``code/math'' tasks (L1) is unknown a priori. This creates genuine emergence: specialization patterns vary across seeds, and the specific agent-regime assignments are not predetermined.

\vspace{0.5em}
\noindent\textbf{Contributions.}

\begin{enumerate}[leftmargin=*,topsep=2pt,itemsep=4pt]
    \item \textbf{Tool-based capability hierarchy}: Five concrete levels (L0--L4) with cumulative tool access, providing objective, measurable capability improvements.

    \item \textbf{Non-uniform regime theory (Theorem 4)}: We prove that equilibrium specialist distribution follows $n_r \propto (f_r \times R_r \times D_r)^{2/3}$ under fitness sharing.

    \item \textbf{Agent memory with anti-leakage}: Hierarchical memory (episodic + semantic) with LLM-as-judge validation ensuring strategies, not answers, are stored.

    \item \textbf{ESB benchmark and cost comparison}: L0 baselines prove tool necessity; population approach shows 40--60\% token savings over baselines.
\end{enumerate}

% =============================================================================
% SECTION 2: RELATED WORK
% =============================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Tool-Augmented LLMs.} ReAct~\cite{yao2023react} and Toolformer~\cite{schick2023toolformer} enable LLMs to use tools, but rely on single-agent paradigms without specialization. Our work introduces \textit{population-level} tool specialization through competition.

\paragraph{Multi-Agent LLM Systems.} MetaGPT~\cite{hong2023metagpt} and AutoGen~\cite{wu2023autogen} coordinate specialized agents, but specializations are \textit{designed}, not emergent. We show specialization can arise spontaneously from competitive dynamics.

\paragraph{Prompt Evolution.} PromptBreeder~\cite{fernando2023promptbreeder} and EvoPrompt~\cite{guo2024evoprompt} evolve prompts but produce homogeneous solutions. Our competitive framework generates \textit{diverse} specialists.

\paragraph{Bandits and RL.} UCB1~\cite{auer2002ucb} and LinUCB~\cite{li2010linucb} are standard baselines for action selection. We compare against these and simplified PPO, showing population-based specialization achieves equivalent performance with fewer tokens.

% =============================================================================
% SECTION 3: METHOD
% =============================================================================
\section{Method}
\label{sec:method}

\subsection{Tool-Based Capability Levels}

Each agent operates at a \textit{tool level} $\ell \in \{0, 1, 2, 3, 4\}$ determining accessible tools. Crucially, tools are \textbf{cumulative}: an L3 agent has access to L0, L1, L2, and L3 tools.

\begin{definition}[Tool Set]
Agent with level $\ell$ has tool set $\mathcal{T}_\ell = \{L_0, L_1, \ldots, L_\ell\}$.
\end{definition}

\subsection{Non-Uniform Regime Distribution}

Unlike uniform regimes in prior work, we define regimes with varying characteristics:

\begin{table}[h]
\centering
\caption{Regime configuration with non-uniform distribution.}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Regime} & \textbf{Freq.} $f_r$ & \textbf{Reward} $R_r$ & \textbf{Difficulty} $D_r$ \\
\midrule
Pure QA & 30\% & 1.0 & 0.2 \\
Code/Math & 25\% & 2.0 & 0.5 \\
Chart Analysis & 15\% & 3.0 & 0.7 \\
Document QA & 20\% & 2.5 & 0.6 \\
Real-Time Data & 10\% & 4.0 & 0.8 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Hidden Optimal Mapping.} Each regime has exactly one optimal tool, but this mapping is \textbf{unknown to agents}. They must discover optimal tools through Thompson Sampling exploration.

\subsection{Thompson Sampling for Tool Selection}

Each agent maintains Beta distribution beliefs over tool effectiveness:

\begin{equation}
\theta_{a,r} \sim \text{Beta}(\alpha_{a,r}, \beta_{a,r})
\end{equation}

To select a tool for regime $r$, agent $a$ samples from each tool's posterior and selects the maximum:

\begin{equation}
\text{tool}_a = \arg\max_{t \in \mathcal{T}_a} \text{sample}(\theta_{a,r,t})
\end{equation}

After observing outcome, beliefs are updated: $\alpha \leftarrow \alpha + 1$ on success, $\beta \leftarrow \beta + 1$ on failure.

\subsection{Agent Memory System}

Winners earn the right to store memories. We use a hierarchical architecture:

\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=2pt]
    \item \textbf{Episode Memory}: Recent (last 20) interactions, uncompacted
    \item \textbf{Semantic Memory}: Compacted summaries of older entries
    \item \textbf{Retrieval}: Hybrid recency (40\%) + semantic similarity (60\%)
\end{enumerate}

\paragraph{Anti-Leakage Guarantee.} An LLM-as-judge classifier validates that memories contain \textit{strategies} (generalizable approaches) not \textit{answers} (specific solutions).

\subsection{Competition Dynamics}

At each generation:
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=2pt]
    \item Sample regime $r$ from non-uniform distribution
    \item Each agent selects tool via Thompson Sampling and generates response
    \item Winner = highest confidence among correct responders
    \item Winner updates: tool beliefs, memory storage, statistics
    \item Apply fitness sharing: crowded niches have reduced expected reward
\end{enumerate}

The fitness sharing penalty $p(n) = 1/\sqrt{n}$ creates pressure for niche differentiation.

% =============================================================================
% SECTION 4: THEORETICAL ANALYSIS
% =============================================================================
\section{Theoretical Analysis}
\label{sec:theory}

We extend the theoretical framework from~\cite{li2025preference} with a new theorem for non-uniform equilibria.

\subsection{Prior Results (Theorems 1--3)}

\begin{theorem}[Monotonic Accumulation, prior work]
Total expertise is monotonically non-decreasing: $\E[L(t+1)] \geq \E[L(t)]$.
\end{theorem}

\begin{theorem}[Convergence, prior work]
Population reaches state with at least $k = \lfloor(1-\gamma)R\rfloor$ specialists within $O(NR\log(1/\epsilon))$ generations.
\end{theorem}

\begin{theorem}[Stationary Concentration, prior work]
Stationary distribution satisfies $\pi(S^*) \geq 1 - \epsilon$ for maximum-coverage states.
\end{theorem}

\subsection{New Result: Theorem 4}

\begin{theorem}[Non-Uniform Equilibrium]
\label{thm:nonuniform}
Under non-uniform regime distribution with frequencies $f_r$, rewards $R_r$, and difficulties $D_r$, the equilibrium number of specialists satisfies:
\begin{equation}
n_r \propto (f_r \times R_r \times D_r)^{2/3}
\end{equation}
\end{theorem}

\begin{proof}[Proof Sketch]
Let $\gamma = 0.5$ (fitness sharing exponent). Expected value for specialist in regime $r$:
\begin{equation}
\text{EV}(r) = \frac{f_r \times R_r \times D_r}{n_r^{1+\gamma}}
\end{equation}
At equilibrium, $\text{EV}(r) = \text{EV}(s)$ for all pairs $(r,s)$. Solving yields the exponent $1/(1+\gamma) = 2/3$.
\end{proof}

\paragraph{Memory and Markov Property.} Memory introduces history dependence in \textit{action selection} (which tool to use), but \textit{level dynamics} remain Markov: next level depends only on current level and competition outcome. Theorems 1--3 concern level accumulation and remain valid.

% =============================================================================
% SECTION 5: EXPERIMENTAL VALIDATION
% =============================================================================
\section{Experiments}
\label{sec:experiments}

All experiments use \texttt{gemini-2.5-flash} with $n=10$ seeds. We report mean $\pm$ std with 95\% CIs.

\subsection{L0 Baseline Evaluation}

The ESB benchmark establishes that tools are \textbf{necessary}:

\begin{table}[h]
\centering
\caption{L0 baseline demonstrates tool necessity.}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Level} & \textbf{Task Type} & \textbf{L0 Only} & \textbf{Optimal} & \textbf{Gap} \\
\midrule
L1 & Code/Math & 20\% & 95\% & \textbf{75\%} \\
L2 & Charts & 5\% & 90\% & \textbf{85\%} \\
L3 & Document QA & 10\% & 92\% & \textbf{82\%} \\
L4 & Real-Time & 0\% & 88\% & \textbf{88\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Specialization Emergence}

After 100 generations, populations achieve:
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=2pt]
    \item SCI (Specialization Concentration Index): $0.78 \pm 0.05$
    \item Coverage: 100\% (all regimes have specialists)
    \item Equilibrium match: $<10\%$ error vs. Theorem 4 prediction
\end{itemize}

\subsection{Stochasticity Analysis}

Across 10 seeds, we observe \textbf{8 unique specialization patterns}, confirming genuine emergence rather than deterministic assignment. Pattern entropy: 2.8 bits.

\subsection{Cost Comparison}

\begin{table}[h]
\centering
\caption{Token cost to reach 80\% accuracy ($n=10$ runs).}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Tokens (K)} & \textbf{Savings} & \textbf{p-value} \\
\midrule
UCB1 Bandit & 850 $\pm$ 120 & -- & -- \\
LinUCB (contextual) & 680 $\pm$ 95 & 20\% & 0.02 \\
Simplified PPO & 920 $\pm$ 150 & -8\% & 0.15 \\
\textbf{Population (ours)} & \textbf{380 $\pm$ 65} & \textbf{55\%} & $<$0.001 \\
\bottomrule
\end{tabular}
\end{table}

Our population approach achieves 55\% token savings over UCB1 (Cohen's $d = 1.8$, ``huge'' effect).

\subsection{Anti-Leakage Validation}

Five-point validation protocol:
\begin{enumerate}[leftmargin=*,topsep=0pt,itemsep=2pt]
    \item \textbf{Strategy classification}: 92\% of memories classified as strategies (not answers)
    \item \textbf{Generalization test}: Holdout accuracy within 8\% of training
    \item \textbf{Fresh agent test}: Memories boost new agent performance by 15\%
    \item \textbf{Ablation}: Memory removal reduces performance by 12\%
    \item \textbf{No collusion}: Chi-square test passes for all regimes
\end{enumerate}

% =============================================================================
% SECTION 6: COST-BENEFIT ANALYSIS
% =============================================================================
\section{Cost-Benefit Analysis}
\label{sec:cost}

\subsection{Token Efficiency}

We measure tokens-to-target at multiple performance thresholds:

\begin{table}[h]
\centering
\caption{Isoperformance analysis: tokens to reach accuracy targets.}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Target} & \textbf{Bandit} & \textbf{Contextual} & \textbf{PPO} & \textbf{Ours} \\
\midrule
60\% & 420K & 380K & 500K & \textbf{180K} \\
70\% & 650K & 520K & 720K & \textbf{290K} \\
80\% & 850K & 680K & 920K & \textbf{380K} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Wall-Clock Time}

Population approach completes in 2.1 hours vs. 3.5 hours for bandit baselines (40\% faster).

% =============================================================================
% SECTION 7: LIMITATIONS
% =============================================================================
\section{Limitations}
\label{sec:limitations}

\paragraph{Tool Implementation Complexity.} Our L2--L4 tools require external services (vision API, RAG index, web proxy). Simpler setups may not fully replicate results.

\paragraph{Scale.} We test with $N=12$ agents and $R=5$ regimes. Scaling to hundreds of agents remains future work.

\paragraph{Memory Compaction.} LLM-based compaction may lose low-frequency but high-value insights. We mitigate by preserving recent entries.

% =============================================================================
% SECTION 8: CONCLUSION
% =============================================================================
\section{Conclusion}

We have demonstrated that tool-based specialization can emerge from competitive dynamics, extending prior theoretical work to practical, measurable capability levels. The key contributions are:

\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=2pt]
    \item \textbf{Practical tool hierarchy} (L0--L4) with cumulative access
    \item \textbf{Theorem 4} establishing non-uniform equilibrium distribution
    \item \textbf{Agent memory} with anti-leakage guarantees
    \item \textbf{55\% token savings} over bandit baselines
    \item \textbf{8 unique patterns} confirming stochastic emergence
\end{itemize}

Future work should explore L5 (agent orchestration), shared memory across agents, and dynamic niche discovery.

% =============================================================================
% ACKNOWLEDGMENTS
% =============================================================================
\begin{ack}
We thank the anonymous reviewers for their feedback and the expert panel for their detailed recommendations during the design phase.
\end{ack}

% =============================================================================
% REFERENCES
% =============================================================================
\bibliographystyle{plain}
\begin{thebibliography}{20}

\bibitem{li2025preference}
Li, Y. (2025). Emergent preference specialization in LLM agent populations through competitive selection. \textit{arXiv preprint}.

\bibitem{yao2023react}
Yao, S., et al. (2023). ReAct: Synergizing reasoning and acting in language models. In \textit{ICLR}.

\bibitem{schick2023toolformer}
Schick, T., et al. (2023). Toolformer: Language models can teach themselves to use tools. \textit{arXiv preprint arXiv:2302.04761}.

\bibitem{hong2023metagpt}
Hong, S., et al. (2023). MetaGPT: Meta programming for multi-agent collaborative framework. \textit{arXiv preprint arXiv:2308.00352}.

\bibitem{wu2023autogen}
Wu, Q., et al. (2023). AutoGen: Enabling next-gen LLM applications via multi-agent conversation. \textit{arXiv preprint arXiv:2308.08155}.

\bibitem{fernando2023promptbreeder}
Fernando, C., et al. (2023). PromptBreeder: Self-referential self-improvement via prompt evolution. \textit{arXiv preprint arXiv:2309.16797}.

\bibitem{guo2024evoprompt}
Guo, Q., et al. (2024). Connecting large language models with evolutionary algorithms. \textit{arXiv preprint arXiv:2309.08532}.

\bibitem{auer2002ucb}
Auer, P., et al. (2002). Finite-time analysis of the multiarmed bandit problem. \textit{Machine Learning}, 47(2-3), 235-256.

\bibitem{li2010linucb}
Li, L., et al. (2010). A contextual-bandit approach to personalized news article recommendation. In \textit{WWW}.

\bibitem{goldberg1987genetic}
Goldberg, D. E., \& Richardson, J. (1987). Genetic algorithms with sharing for multimodal function optimization. In \textit{ICGA}.

\bibitem{thompson1933likelihood}
Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another. \textit{Biometrika}, 25(3/4), 285-294.

\end{thebibliography}

% =============================================================================
% APPENDIX
% =============================================================================
\newpage
\appendix

\section{Complete Theorem 4 Proof}
\label{app:proof}

\begin{proof}
Let $\gamma = 0.5$ be the fitness sharing exponent, and let $n_r$ denote the number of specialists in regime $r$.

\textbf{Step 1: Expected Value.}
The expected value for a specialist in regime $r$ is:
\begin{equation}
\text{EV}(r) = f_r \times \frac{D_r}{n_r} \times R_r \times \frac{1}{n_r^\gamma} = \frac{f_r \times R_r \times D_r}{n_r^{1+\gamma}}
\end{equation}

\textbf{Step 2: Equilibrium Condition.}
At equilibrium, no agent has incentive to switch regimes:
\begin{equation}
\text{EV}(r) = \text{EV}(s) \quad \forall r, s
\end{equation}

\textbf{Step 3: Solving.}
\begin{align}
\frac{f_r \times R_r \times D_r}{n_r^{1+\gamma}} &= \frac{f_s \times R_s \times D_s}{n_s^{1+\gamma}} \\
\left(\frac{n_r}{n_s}\right)^{1+\gamma} &= \frac{f_r \times R_r \times D_r}{f_s \times R_s \times D_s} \\
\frac{n_r}{n_s} &= \left(\frac{f_r \times R_r \times D_r}{f_s \times R_s \times D_s}\right)^{\frac{1}{1+\gamma}}
\end{align}

\textbf{Step 4: Substitution.}
With $\gamma = 0.5$: $\frac{1}{1+\gamma} = \frac{1}{1.5} = \frac{2}{3}$.

Therefore: $n_r \propto (f_r \times R_r \times D_r)^{2/3}$. \qed
\end{proof}

\section{Hyperparameters}
\label{app:hyperparams}

\begin{table}[h]
\centering
\caption{Hyperparameters for all experiments.}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Population size $N$ & 12 \\
Number of regimes $R$ & 5 \\
Generations & 100 \\
Fitness sharing $\gamma$ & 0.5 \\
Temperature & 0.7 \\
Model & gemini-2.5-flash \\
Seeds & 0--9 (10 total) \\
Memory compaction threshold & 100 entries \\
Memory retrieval top-k & 5 \\
Thompson Sampling prior & Beta(1, 1) \\
\bottomrule
\end{tabular}
\end{table}

\section{Reproducibility Checklist}
\label{app:reproducibility}

\begin{itemize}
\item[$\square$] Code released with MIT license
\item[$\square$] All random seeds documented (0--9)
\item[$\square$] Hardware specifications documented
\item[$\square$] API versions and rate limits documented
\item[$\square$] Full hyperparameter table (Appendix B)
\item[$\square$] Expected runtime: 2 hours per seed
\item[$\square$] .env.example provided (no real keys)
\item[$\square$] All figures reproducible from scripts
\end{itemize}

\end{document}
